{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>336058.90</td>\n",
       "      <td>C1401303763</td>\n",
       "      <td>10553134.58</td>\n",
       "      <td>10889193.47</td>\n",
       "      <td>C691717464</td>\n",
       "      <td>635888.08</td>\n",
       "      <td>299829.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>166351.66</td>\n",
       "      <td>C1991628344</td>\n",
       "      <td>589271.93</td>\n",
       "      <td>755623.59</td>\n",
       "      <td>C191462571</td>\n",
       "      <td>2212715.10</td>\n",
       "      <td>2046363.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>346</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>56937.15</td>\n",
       "      <td>C1893138634</td>\n",
       "      <td>7076.00</td>\n",
       "      <td>64013.15</td>\n",
       "      <td>C571753084</td>\n",
       "      <td>1238133.10</td>\n",
       "      <td>1181195.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>40887.55</td>\n",
       "      <td>C80769932</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C21991437</td>\n",
       "      <td>2278589.76</td>\n",
       "      <td>2319477.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>41289.13</td>\n",
       "      <td>C104957723</td>\n",
       "      <td>114781.00</td>\n",
       "      <td>73491.87</td>\n",
       "      <td>C1026434684</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41289.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type     amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0    17   CASH_IN  336058.90  C1401303763    10553134.58     10889193.47   \n",
       "1   540   CASH_IN  166351.66  C1991628344      589271.93       755623.59   \n",
       "2   346   CASH_IN   56937.15  C1893138634        7076.00        64013.15   \n",
       "3   400  CASH_OUT   40887.55    C80769932           0.00            0.00   \n",
       "4   134  CASH_OUT   41289.13   C104957723      114781.00        73491.87   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
       "0   C691717464       635888.08       299829.18        0               0  \n",
       "1   C191462571      2212715.10      2046363.43        0               0  \n",
       "2   C571753084      1238133.10      1181195.94        0               0  \n",
       "3    C21991437      2278589.76      2319477.30        0               0  \n",
       "4  C1026434684            0.00        41289.13        0               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   step            100000 non-null  int64  \n",
      " 1   type            100000 non-null  object \n",
      " 2   amount          100000 non-null  float64\n",
      " 3   nameOrig        100000 non-null  object \n",
      " 4   oldbalanceOrg   100000 non-null  float64\n",
      " 5   newbalanceOrig  100000 non-null  float64\n",
      " 6   nameDest        100000 non-null  object \n",
      " 7   oldbalanceDest  100000 non-null  float64\n",
      " 8   newbalanceDest  100000 non-null  float64\n",
      " 9   isFraud         100000 non-null  int64  \n",
      " 10  isFlaggedFraud  100000 non-null  int64  \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 8.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   step            10000 non-null  int64  \n",
      " 1   type            10000 non-null  object \n",
      " 2   amount          10000 non-null  float64\n",
      " 3   nameOrig        10000 non-null  object \n",
      " 4   oldbalanceOrg   10000 non-null  float64\n",
      " 5   newbalanceOrig  10000 non-null  float64\n",
      " 6   nameDest        10000 non-null  object \n",
      " 7   oldbalanceDest  10000 non-null  float64\n",
      " 8   newbalanceDest  10000 non-null  float64\n",
      " 9   isFraud         10000 non-null  int64  \n",
      " 10  isFlaggedFraud  10000 non-null  int64  \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data_new = data.head(100000)\n",
    "data_new.info()\n",
    "test_new = test_data.head(10000)\n",
    "test_new.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_8308\\857976918.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_new['type'] = type_le.transform(data_new['type'])\n",
      "C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_8308\\857976918.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_new['nameOrig'] = name_orig.transform(data_new['nameOrig'])\n",
      "C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_8308\\857976918.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_new['nameDest'] = name_dest.transform(data_new['nameDest'])\n",
      "C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_8308\\857976918.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_new['type'] = type_le.transform(test_new['type'])\n",
      "C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_8308\\857976918.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_new['nameOrig'] = name_orig.transform(test_new['nameOrig'])\n",
      "C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_8308\\857976918.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_new['nameDest'] = name_dest.transform(test_new['nameDest'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((100000, 10), (100000, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting Categorical Feature into Numerical Feature\n",
    "le = LabelEncoder()\n",
    "\n",
    "#for train data\n",
    "type_le = le.fit(data_new['type'])\n",
    "data_new['type'] = type_le.transform(data_new['type'])\n",
    "\n",
    "name_orig = le.fit(data_new['nameOrig'])\n",
    "data_new['nameOrig'] = name_orig.transform(data_new['nameOrig'])\n",
    "\n",
    "name_dest = le.fit(data_new['nameDest'])\n",
    "data_new['nameDest'] = name_dest.transform(data_new['nameDest'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#for test data \n",
    "test_new['type'] = type_le.fit_transform(test_new['type'])\n",
    "test_new['nameOrig'] = name_orig.transform(test_new['nameOrig'])\n",
    "test_new['nameDest'] = name_dest.transform(test_new['nameDest'])\n",
    "\"\"\"\n",
    "\n",
    "#for test data\n",
    "type_le = le.fit(test_new['type'])\n",
    "test_new['type'] = type_le.transform(test_new['type'])\n",
    "\n",
    "name_orig = le.fit(test_new['nameOrig'])\n",
    "test_new['nameOrig'] = name_orig.transform(test_new['nameOrig'])\n",
    "\n",
    "name_dest = le.fit(test_new['nameDest'])\n",
    "test_new['nameDest'] = name_dest.transform(test_new['nameDest'])\n",
    "\n",
    "#splitting data into train and test\n",
    "X = data_new.loc[:, data_new.columns != 'isFraud']\n",
    "y = data_new.loc[:, data_new.columns == 'isFraud']\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "#data_new.info()\n",
    "#test_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.20875 | train_accuracy: 0.9985  | valid_accuracy: 0.9988  |  0:00:10s\n",
      "epoch 1  | loss: 0.02643 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:00:20s\n",
      "epoch 2  | loss: 0.01618 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:00:31s\n",
      "epoch 3  | loss: 0.01358 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:00:41s\n",
      "epoch 4  | loss: 0.01278 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:00:52s\n",
      "epoch 5  | loss: 0.01205 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:01:03s\n",
      "epoch 6  | loss: 0.01196 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:01:14s\n",
      "epoch 7  | loss: 0.01149 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:01:24s\n",
      "epoch 8  | loss: 0.01124 | train_accuracy: 0.99851 | valid_accuracy: 0.99885 |  0:01:34s\n",
      "epoch 9  | loss: 0.01058 | train_accuracy: 0.99852 | valid_accuracy: 0.9988  |  0:01:45s\n",
      "epoch 10 | loss: 0.00988 | train_accuracy: 0.99855 | valid_accuracy: 0.9989  |  0:01:56s\n",
      "epoch 11 | loss: 0.00955 | train_accuracy: 0.9986  | valid_accuracy: 0.99905 |  0:02:07s\n",
      "epoch 12 | loss: 0.00937 | train_accuracy: 0.99832 | valid_accuracy: 0.9985  |  0:02:17s\n",
      "epoch 13 | loss: 0.00933 | train_accuracy: 0.9984  | valid_accuracy: 0.99885 |  0:02:27s\n",
      "epoch 14 | loss: 0.00893 | train_accuracy: 0.99836 | valid_accuracy: 0.9991  |  0:02:38s\n",
      "epoch 15 | loss: 0.00872 | train_accuracy: 0.99855 | valid_accuracy: 0.9991  |  0:02:48s\n",
      "epoch 16 | loss: 0.00844 | train_accuracy: 0.99862 | valid_accuracy: 0.99915 |  0:02:57s\n",
      "epoch 17 | loss: 0.00844 | train_accuracy: 0.99854 | valid_accuracy: 0.99915 |  0:03:08s\n",
      "epoch 18 | loss: 0.0085  | train_accuracy: 0.99865 | valid_accuracy: 0.99915 |  0:03:19s\n",
      "epoch 19 | loss: 0.00787 | train_accuracy: 0.99866 | valid_accuracy: 0.9992  |  0:03:31s\n",
      "epoch 20 | loss: 0.00771 | train_accuracy: 0.99869 | valid_accuracy: 0.99915 |  0:03:44s\n",
      "epoch 21 | loss: 0.00812 | train_accuracy: 0.99868 | valid_accuracy: 0.99915 |  0:03:54s\n",
      "epoch 22 | loss: 0.00764 | train_accuracy: 0.99868 | valid_accuracy: 0.99905 |  0:04:05s\n",
      "epoch 23 | loss: 0.00767 | train_accuracy: 0.99868 | valid_accuracy: 0.99915 |  0:04:16s\n",
      "epoch 24 | loss: 0.0077  | train_accuracy: 0.99869 | valid_accuracy: 0.9992  |  0:04:29s\n",
      "epoch 25 | loss: 0.00709 | train_accuracy: 0.99872 | valid_accuracy: 0.9992  |  0:04:40s\n",
      "epoch 26 | loss: 0.00727 | train_accuracy: 0.9987  | valid_accuracy: 0.9992  |  0:04:54s\n",
      "epoch 27 | loss: 0.00711 | train_accuracy: 0.99878 | valid_accuracy: 0.99925 |  0:05:04s\n",
      "epoch 28 | loss: 0.0068  | train_accuracy: 0.99879 | valid_accuracy: 0.99925 |  0:05:14s\n",
      "epoch 29 | loss: 0.00716 | train_accuracy: 0.99881 | valid_accuracy: 0.99925 |  0:05:24s\n",
      "epoch 30 | loss: 0.00682 | train_accuracy: 0.9987  | valid_accuracy: 0.9992  |  0:05:33s\n",
      "epoch 31 | loss: 0.00686 | train_accuracy: 0.99879 | valid_accuracy: 0.99925 |  0:05:43s\n",
      "epoch 32 | loss: 0.00654 | train_accuracy: 0.99885 | valid_accuracy: 0.9993  |  0:05:53s\n",
      "epoch 33 | loss: 0.00667 | train_accuracy: 0.99881 | valid_accuracy: 0.99925 |  0:06:02s\n",
      "epoch 34 | loss: 0.00668 | train_accuracy: 0.99886 | valid_accuracy: 0.9993  |  0:06:12s\n",
      "epoch 35 | loss: 0.0065  | train_accuracy: 0.99888 | valid_accuracy: 0.9993  |  0:06:22s\n",
      "epoch 36 | loss: 0.00632 | train_accuracy: 0.99886 | valid_accuracy: 0.99925 |  0:06:31s\n",
      "epoch 37 | loss: 0.00621 | train_accuracy: 0.99874 | valid_accuracy: 0.99925 |  0:06:41s\n",
      "epoch 38 | loss: 0.00626 | train_accuracy: 0.99885 | valid_accuracy: 0.9993  |  0:06:51s\n",
      "epoch 39 | loss: 0.0066  | train_accuracy: 0.99888 | valid_accuracy: 0.9993  |  0:07:01s\n",
      "epoch 40 | loss: 0.00648 | train_accuracy: 0.99886 | valid_accuracy: 0.99925 |  0:07:12s\n",
      "epoch 41 | loss: 0.00627 | train_accuracy: 0.99888 | valid_accuracy: 0.99935 |  0:07:22s\n",
      "epoch 42 | loss: 0.00608 | train_accuracy: 0.99888 | valid_accuracy: 0.99925 |  0:07:32s\n",
      "epoch 43 | loss: 0.00611 | train_accuracy: 0.99886 | valid_accuracy: 0.9993  |  0:07:43s\n",
      "epoch 44 | loss: 0.00562 | train_accuracy: 0.99884 | valid_accuracy: 0.9993  |  0:07:53s\n",
      "epoch 45 | loss: 0.00585 | train_accuracy: 0.99888 | valid_accuracy: 0.9993  |  0:08:04s\n",
      "epoch 46 | loss: 0.00589 | train_accuracy: 0.99881 | valid_accuracy: 0.9993  |  0:08:14s\n",
      "epoch 47 | loss: 0.0058  | train_accuracy: 0.99881 | valid_accuracy: 0.99925 |  0:08:24s\n",
      "epoch 48 | loss: 0.00653 | train_accuracy: 0.99882 | valid_accuracy: 0.99925 |  0:08:35s\n",
      "epoch 49 | loss: 0.00613 | train_accuracy: 0.99886 | valid_accuracy: 0.9993  |  0:08:46s\n",
      "epoch 50 | loss: 0.00587 | train_accuracy: 0.99888 | valid_accuracy: 0.9993  |  0:08:56s\n",
      "epoch 51 | loss: 0.00568 | train_accuracy: 0.99885 | valid_accuracy: 0.99935 |  0:09:07s\n",
      "epoch 52 | loss: 0.0058  | train_accuracy: 0.99885 | valid_accuracy: 0.9993  |  0:09:17s\n",
      "epoch 53 | loss: 0.00561 | train_accuracy: 0.9989  | valid_accuracy: 0.99935 |  0:09:27s\n",
      "epoch 54 | loss: 0.00562 | train_accuracy: 0.99892 | valid_accuracy: 0.99935 |  0:09:37s\n",
      "epoch 55 | loss: 0.00549 | train_accuracy: 0.99886 | valid_accuracy: 0.9993  |  0:09:47s\n",
      "epoch 56 | loss: 0.00551 | train_accuracy: 0.99892 | valid_accuracy: 0.9993  |  0:09:57s\n",
      "epoch 57 | loss: 0.00554 | train_accuracy: 0.99892 | valid_accuracy: 0.9993  |  0:10:07s\n",
      "epoch 58 | loss: 0.00556 | train_accuracy: 0.99892 | valid_accuracy: 0.9993  |  0:10:16s\n",
      "epoch 59 | loss: 0.00524 | train_accuracy: 0.99892 | valid_accuracy: 0.99935 |  0:10:26s\n",
      "epoch 60 | loss: 0.00558 | train_accuracy: 0.9989  | valid_accuracy: 0.99935 |  0:10:35s\n",
      "epoch 61 | loss: 0.0054  | train_accuracy: 0.99888 | valid_accuracy: 0.99935 |  0:10:45s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_valid_accuracy = 0.99935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.20895 | train_accuracy: 0.99852 | valid_accuracy: 0.9986  |  0:00:09s\n",
      "epoch 1  | loss: 0.0261  | train_accuracy: 0.99858 | valid_accuracy: 0.9986  |  0:00:19s\n",
      "epoch 2  | loss: 0.01575 | train_accuracy: 0.99858 | valid_accuracy: 0.9986  |  0:00:28s\n",
      "epoch 3  | loss: 0.01321 | train_accuracy: 0.99858 | valid_accuracy: 0.9986  |  0:00:38s\n",
      "epoch 4  | loss: 0.01197 | train_accuracy: 0.99858 | valid_accuracy: 0.9986  |  0:00:48s\n",
      "epoch 5  | loss: 0.01182 | train_accuracy: 0.99858 | valid_accuracy: 0.9986  |  0:00:57s\n",
      "epoch 6  | loss: 0.01137 | train_accuracy: 0.99858 | valid_accuracy: 0.99865 |  0:01:07s\n",
      "epoch 7  | loss: 0.01027 | train_accuracy: 0.9986  | valid_accuracy: 0.9986  |  0:01:17s\n",
      "epoch 8  | loss: 0.0099  | train_accuracy: 0.99864 | valid_accuracy: 0.99855 |  0:01:27s\n",
      "epoch 9  | loss: 0.00965 | train_accuracy: 0.99865 | valid_accuracy: 0.99865 |  0:01:36s\n",
      "epoch 10 | loss: 0.00913 | train_accuracy: 0.9987  | valid_accuracy: 0.9986  |  0:01:46s\n",
      "epoch 11 | loss: 0.00893 | train_accuracy: 0.99872 | valid_accuracy: 0.9986  |  0:01:55s\n",
      "epoch 12 | loss: 0.00884 | train_accuracy: 0.99871 | valid_accuracy: 0.99865 |  0:02:05s\n",
      "epoch 13 | loss: 0.00857 | train_accuracy: 0.99874 | valid_accuracy: 0.99865 |  0:23:42s\n",
      "epoch 14 | loss: 0.00819 | train_accuracy: 0.99875 | valid_accuracy: 0.99865 |  0:23:52s\n",
      "epoch 15 | loss: 0.00819 | train_accuracy: 0.99875 | valid_accuracy: 0.99865 |  0:24:02s\n",
      "epoch 16 | loss: 0.00778 | train_accuracy: 0.99875 | valid_accuracy: 0.99865 |  0:24:13s\n",
      "epoch 17 | loss: 0.00729 | train_accuracy: 0.99875 | valid_accuracy: 0.99865 |  0:24:24s\n",
      "epoch 18 | loss: 0.00745 | train_accuracy: 0.99878 | valid_accuracy: 0.9986  |  0:24:34s\n",
      "epoch 19 | loss: 0.00739 | train_accuracy: 0.99875 | valid_accuracy: 0.99865 |  0:24:45s\n",
      "epoch 20 | loss: 0.00731 | train_accuracy: 0.99876 | valid_accuracy: 0.99865 |  0:24:55s\n",
      "epoch 21 | loss: 0.0073  | train_accuracy: 0.99879 | valid_accuracy: 0.99865 |  0:25:05s\n",
      "epoch 22 | loss: 0.00698 | train_accuracy: 0.9988  | valid_accuracy: 0.99875 |  0:25:16s\n",
      "epoch 23 | loss: 0.00689 | train_accuracy: 0.99885 | valid_accuracy: 0.9988  |  0:25:26s\n",
      "epoch 24 | loss: 0.00707 | train_accuracy: 0.99878 | valid_accuracy: 0.99865 |  0:25:36s\n",
      "epoch 25 | loss: 0.00684 | train_accuracy: 0.99882 | valid_accuracy: 0.9987  |  0:25:46s\n",
      "epoch 26 | loss: 0.00683 | train_accuracy: 0.99881 | valid_accuracy: 0.99875 |  0:25:55s\n",
      "epoch 27 | loss: 0.00667 | train_accuracy: 0.99878 | valid_accuracy: 0.9987  |  0:26:05s\n",
      "epoch 28 | loss: 0.00678 | train_accuracy: 0.99886 | valid_accuracy: 0.99875 |  0:26:14s\n",
      "epoch 29 | loss: 0.0068  | train_accuracy: 0.99879 | valid_accuracy: 0.99865 |  0:26:23s\n",
      "epoch 30 | loss: 0.00646 | train_accuracy: 0.99886 | valid_accuracy: 0.9988  |  0:26:33s\n",
      "epoch 31 | loss: 0.00649 | train_accuracy: 0.99886 | valid_accuracy: 0.99875 |  0:26:42s\n",
      "epoch 32 | loss: 0.00636 | train_accuracy: 0.99886 | valid_accuracy: 0.99875 |  0:26:51s\n",
      "epoch 33 | loss: 0.0068  | train_accuracy: 0.9989  | valid_accuracy: 0.9988  |  0:27:00s\n",
      "epoch 34 | loss: 0.00667 | train_accuracy: 0.99889 | valid_accuracy: 0.9988  |  0:27:09s\n",
      "epoch 35 | loss: 0.0064  | train_accuracy: 0.99894 | valid_accuracy: 0.99885 |  0:27:19s\n",
      "epoch 36 | loss: 0.0061  | train_accuracy: 0.99886 | valid_accuracy: 0.99885 |  0:27:28s\n",
      "epoch 37 | loss: 0.00667 | train_accuracy: 0.99888 | valid_accuracy: 0.99885 |  0:27:37s\n",
      "epoch 38 | loss: 0.0064  | train_accuracy: 0.99888 | valid_accuracy: 0.99885 |  0:27:46s\n",
      "epoch 39 | loss: 0.00615 | train_accuracy: 0.99889 | valid_accuracy: 0.9988  |  0:27:56s\n",
      "epoch 40 | loss: 0.00599 | train_accuracy: 0.99889 | valid_accuracy: 0.9988  |  0:28:05s\n",
      "epoch 41 | loss: 0.00632 | train_accuracy: 0.99892 | valid_accuracy: 0.99885 |  0:28:14s\n",
      "epoch 42 | loss: 0.00605 | train_accuracy: 0.99891 | valid_accuracy: 0.99885 |  0:28:24s\n",
      "epoch 43 | loss: 0.00592 | train_accuracy: 0.99889 | valid_accuracy: 0.9988  |  0:28:33s\n",
      "epoch 44 | loss: 0.00603 | train_accuracy: 0.9989  | valid_accuracy: 0.9988  |  0:28:43s\n",
      "epoch 45 | loss: 0.00633 | train_accuracy: 0.99898 | valid_accuracy: 0.99885 |  0:28:53s\n",
      "epoch 46 | loss: 0.00599 | train_accuracy: 0.99889 | valid_accuracy: 0.9988  |  0:38:47s\n",
      "epoch 47 | loss: 0.00585 | train_accuracy: 0.99896 | valid_accuracy: 0.99885 |  0:38:59s\n",
      "epoch 48 | loss: 0.00607 | train_accuracy: 0.99898 | valid_accuracy: 0.99885 |  0:39:10s\n",
      "epoch 49 | loss: 0.00575 | train_accuracy: 0.99901 | valid_accuracy: 0.99885 |  0:39:21s\n",
      "epoch 50 | loss: 0.00574 | train_accuracy: 0.99895 | valid_accuracy: 0.99885 |  0:39:31s\n",
      "epoch 51 | loss: 0.00547 | train_accuracy: 0.99901 | valid_accuracy: 0.99885 |  0:39:42s\n",
      "epoch 52 | loss: 0.00563 | train_accuracy: 0.99895 | valid_accuracy: 0.99885 |  0:39:52s\n",
      "epoch 53 | loss: 0.00567 | train_accuracy: 0.99895 | valid_accuracy: 0.99885 |  0:40:02s\n",
      "epoch 54 | loss: 0.00551 | train_accuracy: 0.99898 | valid_accuracy: 0.99885 |  0:40:13s\n",
      "epoch 55 | loss: 0.00572 | train_accuracy: 0.99896 | valid_accuracy: 0.99885 |  0:40:23s\n",
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 35 and best_valid_accuracy = 0.99885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.21407 | train_accuracy: 0.99861 | valid_accuracy: 0.99835 |  0:00:10s\n",
      "epoch 1  | loss: 0.02642 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:00:20s\n",
      "epoch 2  | loss: 0.01561 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:00:31s\n",
      "epoch 3  | loss: 0.01323 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:00:40s\n",
      "epoch 4  | loss: 0.01243 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:00:50s\n",
      "epoch 5  | loss: 0.01181 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:00:59s\n",
      "epoch 6  | loss: 0.01124 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:01:09s\n",
      "epoch 7  | loss: 0.01103 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:01:18s\n",
      "epoch 8  | loss: 0.01078 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:01:27s\n",
      "epoch 9  | loss: 0.01016 | train_accuracy: 0.99862 | valid_accuracy: 0.9984  |  0:01:37s\n",
      "epoch 10 | loss: 0.01023 | train_accuracy: 0.99864 | valid_accuracy: 0.99845 |  0:01:46s\n",
      "epoch 11 | loss: 0.00931 | train_accuracy: 0.99869 | valid_accuracy: 0.9985  |  0:01:56s\n",
      "epoch 12 | loss: 0.00897 | train_accuracy: 0.99875 | valid_accuracy: 0.9985  |  0:02:06s\n",
      "epoch 13 | loss: 0.00853 | train_accuracy: 0.99876 | valid_accuracy: 0.9985  |  0:02:15s\n",
      "epoch 14 | loss: 0.00836 | train_accuracy: 0.9988  | valid_accuracy: 0.9985  |  0:02:25s\n",
      "epoch 15 | loss: 0.00804 | train_accuracy: 0.99882 | valid_accuracy: 0.9985  |  0:02:35s\n",
      "epoch 16 | loss: 0.0078  | train_accuracy: 0.9988  | valid_accuracy: 0.9985  |  0:02:45s\n",
      "epoch 17 | loss: 0.00762 | train_accuracy: 0.99882 | valid_accuracy: 0.9985  |  0:02:55s\n",
      "epoch 18 | loss: 0.0074  | train_accuracy: 0.99882 | valid_accuracy: 0.9985  |  0:03:04s\n",
      "epoch 19 | loss: 0.00706 | train_accuracy: 0.99884 | valid_accuracy: 0.9985  |  0:03:14s\n",
      "epoch 20 | loss: 0.00751 | train_accuracy: 0.99884 | valid_accuracy: 0.9985  |  0:03:23s\n",
      "epoch 21 | loss: 0.00677 | train_accuracy: 0.99882 | valid_accuracy: 0.9985  |  0:03:33s\n",
      "epoch 22 | loss: 0.00652 | train_accuracy: 0.99882 | valid_accuracy: 0.9985  |  0:03:44s\n",
      "epoch 23 | loss: 0.0066  | train_accuracy: 0.99882 | valid_accuracy: 0.99855 |  0:03:56s\n",
      "epoch 24 | loss: 0.00688 | train_accuracy: 0.99882 | valid_accuracy: 0.9985  |  0:08:48s\n",
      "epoch 25 | loss: 0.00676 | train_accuracy: 0.99884 | valid_accuracy: 0.9985  |  0:09:02s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[39m#model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     tab_clf \u001b[39m=\u001b[39m TabNetClassifier(optimizer_fn\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam,\n\u001b[0;32m     15\u001b[0m                            optimizer_params\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m),\n\u001b[0;32m     16\u001b[0m                             scheduler_params\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mstep_size\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m0.9\u001b[39m}, \u001b[39m# how to use learning rate scheduler\u001b[39;00m\n\u001b[0;32m     17\u001b[0m                             scheduler_fn\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR,\n\u001b[0;32m     18\u001b[0m                             mask_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mentmax\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# \"sparsemax\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m                             )\n\u001b[1;32m---> 21\u001b[0m     tab_clf\u001b[39m.\u001b[39;49mfit(X_train, y_train,\n\u001b[0;32m     22\u001b[0m                 eval_set\u001b[39m=\u001b[39;49m[(X_train, y_train), (X_test, y_test)],\n\u001b[0;32m     23\u001b[0m                 eval_name\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     24\u001b[0m                 eval_metric\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     25\u001b[0m                 max_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m                 batch_size \u001b[39m=\u001b[39;49m \u001b[39m512\u001b[39;49m,\n\u001b[0;32m     27\u001b[0m                 drop_last\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m                 )\n\u001b[0;32m     29\u001b[0m     cv_scores\u001b[39m.\u001b[39mappend(tab_clf\u001b[39m.\u001b[39mbest_cost)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(cv_scores)\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:241\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[0;32m    243\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:457\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m    455\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 457\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X, y)\n\u001b[0;32m    459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    461\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda-new\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:502\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    499\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_sparse \u001b[39m*\u001b[39m M_loss\n\u001b[0;32m    501\u001b[0m \u001b[39m# Perform backward pass and optimization\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_value:\n\u001b[0;32m    504\u001b[0m     clip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_value)\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda-new\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda-new\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda-new\\lib\\site-packages\\torch\\autograd\\function.py:257\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m--> 257\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m    258\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    259\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "y= y.flatten()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    #model\n",
    "    tab_clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                           optimizer_params=dict(lr=1e-3),\n",
    "                            scheduler_params={\"step_size\":10, \"gamma\":0.9}, # how to use learning rate scheduler\n",
    "                            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                            mask_type='entmax' # \"sparsemax\"\n",
    "                            )\n",
    "    \n",
    "    tab_clf.fit(X_train, y_train,\n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                eval_name=['train', 'valid'],\n",
    "                eval_metric=['accuracy'],\n",
    "                max_epochs=20, patience=20,\n",
    "                batch_size = 512,\n",
    "                drop_last=False\n",
    "                )\n",
    "    cv_scores.append(tab_clf.best_cost)\n",
    "\n",
    "print(cv_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
